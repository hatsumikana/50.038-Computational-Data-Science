{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw6D4-VfgpoC"
      },
      "source": [
        "## Load the IMDB dataset and create the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8ldWVj-gpoF",
        "outputId": "2bb97bbe-259b-4dac-a123-087ff9964bfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84.1M/84.1M [00:03<00:00, 27.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "from torchtext.datasets import IMDB\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter, OrderedDict\n",
        "from torchtext.vocab import vocab\n",
        "\n",
        "EMBEDDING_DIM=50\n",
        "VOCAB_SIZE=20000\n",
        "\n",
        "# Get IMDB dataset\n",
        "imdb = IMDB(split='train')\n",
        "\n",
        "# Load English tokenizer, tagger, parser and NER\n",
        "tokenizer = get_tokenizer('spacy', language='en')\n",
        "\n",
        "# build the vocab\n",
        "counter = Counter()\n",
        "for i, (label, line) in enumerate(imdb):\n",
        "    counter.update(tokenizer(line))\n",
        "\n",
        "ordered_dict = OrderedDict(counter.most_common()[:VOCAB_SIZE])\n",
        "vocab = vocab(ordered_dict)\n",
        "\n",
        "# insert special tokens and set default index to 'unknown'\n",
        "vocab.insert_token('<PAD>', 0)\n",
        "vocab.insert_token('<UNK>', 1)\n",
        "vocab.set_default_index(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfSjPPpQgpoH"
      },
      "source": [
        "## Create embedding vectors from GloVe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2D8ylifgpoI",
        "outputId": "5b050342-f4f6-41f4-c754-24652f9d8010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:40, 5.36MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:14<00:00, 28467.23it/s]\n"
          ]
        }
      ],
      "source": [
        "import torchtext as text\n",
        "\n",
        "# load glove embeddings\n",
        "vec = text.vocab.GloVe(name='6B', dim=50)\n",
        "# create the embedding matrix, a torch tensor in the shape (num_words+1, embedding_dim)\n",
        "word_emb = vec.get_vecs_by_tokens(vocab.get_itos())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa53xG_HgpoI"
      },
      "source": [
        "## Build up train/test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DeUBQXuagpoI"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# transform input text and label to ids\n",
        "def process_text(text):\n",
        "    return vocab(tokenizer(text))\n",
        "\n",
        "label_to_ids = {'pos':0, 'neg':1}\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, lengths = [], [], []\n",
        "    for (_label, _text) in batch:\n",
        "        label_list.append(label_to_ids[_label])\n",
        "        processed_text = torch.tensor(process_text(_text), dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "        lengths.append(processed_text.size(0))\n",
        "    # label must be in the same size as target\n",
        "    label_list = torch.tensor(label_list, dtype=torch.float)[:,None]\n",
        "\n",
        "    text_list = pad_sequence(text_list, batch_first=True)\n",
        "    lengths = torch.tensor(lengths, dtype=torch.float)\n",
        "    return label_list.to(device), text_list.to(device), lengths.to(device)\n",
        "\n",
        "train_iter, test_iter = IMDB()\n",
        "\n",
        "# transform datasets iterator into map style so that they can be repeatedly loaded in a new epoch\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=128,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=128,\n",
        "                             shuffle=True, collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "szVqflWHgpoJ"
      },
      "outputs": [],
      "source": [
        "# logistic model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, word_vec, embed_dim):\n",
        "        super().__init__()\n",
        "        # embeddingbag outputs the average of all the words in a sentence\n",
        "        self.embedding = nn.Embedding(*(word_vec.size())).from_pretrained(word_vec, freeze=False)\n",
        "        self.fc = nn.Linear(embed_dim, 1)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize network parameters \n",
        "        \"\"\"\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, lengths):\n",
        "        embedded = self.embedding(text) # (batch_size, sent_len, emb_size)\n",
        "        embedded = embedded.sum(dim = 1) / lengths[:, None] # (add one axis)\n",
        "        return torch.sigmoid(self.fc(embedded))\n",
        "\n",
        "class LSTMcustom(nn.Module):\n",
        "    def __init__(self, word_vec, embed_dim):\n",
        "        super().__init__()\n",
        "        # embeddingbag outputs the average of all the words in a sentence\n",
        "        self.embedding = nn.Embedding(*(word_vec.size())).from_pretrained(word_vec, freeze=False)\n",
        "        # Initialize LSTM model. The arguments are in this order input_dim, hidden_dim, n_layers\n",
        "        \n",
        "        # Q1: Create a 1D-CNN and feed the output to the next LSTM layer. Use torch.nn.Conv1d. Set number of output channels to 20. The number of input channels should be equal to the embedding dim. The kernel size is 2.\n",
        "\n",
        "        # Hint: A CNN accepts input as (batch_size, embedding_dim, sequence_len) and returns output as (batch_size, out_embedding_dim, out_sequence_len). Note that original sequence and embedding_dim can change after you apply CNN. Hence you will need to transpose your input before you feed that to CNN. Use torch.permute or torch.transpose\n",
        "        #       More details here: https://pytorch.org/docs/stable/generated/torch.permute.html\n",
        "        self.cnn = torch.nn.Conv1d(embed_dim, 20, 2)\n",
        "        # Q2: Determine the input shape of this LSTM layer.\n",
        "        self.lstm = nn.LSTM(20, 200, 1, bidirectional=False, batch_first = True)\n",
        "        \n",
        "        # Q3 (optional): Implement a dot product based attention layer on top of the LSTM layer. Suppose the output of the LSTM is 3D shaped M -> (batch_size, seq_len, output_size), initilize a parameter matrix W using FC layer. Multiply W and M followed by tanh and then softmax activation. Hint: Use torch.softmax(torch.tanh(nn.Linear())). Visualize the attention values. Take the weighted sum of the lstm outputs based on the attention weights and this is going to be the final output of the attention layer.\n",
        "        \n",
        "        self.fc = nn.Linear(200, 1)\n",
        "                \n",
        "    def forward(self, text, lengths):\n",
        "        embedded = self.embedding(text) # (batch_size, sent_len, emb_size)\n",
        "        embedded = embedded.permute(0, 2, 1)\n",
        "        cnn_out = self.cnn(embedded)\n",
        "        cnn_out = torch.permute(cnn_out, (0,2,1))\n",
        "        lstm_out,_ = self.lstm(cnn_out) # lstm_out is a 3d tensor (batch_size, seq_len, output_size). If you have a bidirectional LSTM, the outputsize will be 2*output_size\n",
        "        \n",
        "        lstm_out = lstm_out[:, -1, :] # clipping the lstm_output. Remove it if you implement the attention layer.\n",
        "        \n",
        "        \n",
        "        # Q4 (optional): Replace this sigmoid with softmax. You may need to change the dataloader as well.\n",
        "        return torch.sigmoid(self.fc(lstm_out))   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "d6Bi5-j_gpoK"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 50\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text, lengths) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        # forward propagation\n",
        "        predicted_label = model(text, lengths)\n",
        "        # calculate loss and backpropagate to model paramters\n",
        "        loss = criterion(predicted_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        # update parameters by stepping the optimizer\n",
        "        optimizer.step()\n",
        "        total_acc += ((predicted_label > 0.5) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, lengths) in enumerate(dataloader):\n",
        "            predicted_label = model(text, lengths)\n",
        "            loss = criterion(predicted_label, label)\n",
        "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NripdSiRgpoL",
        "outputId": "7090d6d0-605f-48ea-f141-4aad0736aa7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |    50/  196 batches | accuracy    0.495\n",
            "| epoch   1 |   100/  196 batches | accuracy    0.493\n",
            "| epoch   1 |   150/  196 batches | accuracy    0.497\n",
            "| epoch   2 |    50/  196 batches | accuracy    0.500\n",
            "| epoch   2 |   100/  196 batches | accuracy    0.507\n",
            "| epoch   2 |   150/  196 batches | accuracy    0.498\n",
            "| epoch   3 |    50/  196 batches | accuracy    0.495\n",
            "| epoch   3 |   100/  196 batches | accuracy    0.506\n",
            "| epoch   3 |   150/  196 batches | accuracy    0.493\n",
            "| epoch   4 |    50/  196 batches | accuracy    0.497\n",
            "| epoch   4 |   100/  196 batches | accuracy    0.500\n",
            "| epoch   4 |   150/  196 batches | accuracy    0.505\n",
            "| epoch   5 |    50/  196 batches | accuracy    0.505\n",
            "| epoch   5 |   100/  196 batches | accuracy    0.494\n",
            "| epoch   5 |   150/  196 batches | accuracy    0.499\n",
            "| epoch   6 |    50/  196 batches | accuracy    0.493\n",
            "| epoch   6 |   100/  196 batches | accuracy    0.499\n",
            "| epoch   6 |   150/  196 batches | accuracy    0.507\n",
            "| epoch   7 |    50/  196 batches | accuracy    0.499\n",
            "| epoch   7 |   100/  196 batches | accuracy    0.506\n",
            "| epoch   7 |   150/  196 batches | accuracy    0.497\n",
            "| epoch   8 |    50/  196 batches | accuracy    0.495\n",
            "| epoch   8 |   100/  196 batches | accuracy    0.501\n",
            "| epoch   8 |   150/  196 batches | accuracy    0.506\n",
            "| epoch   9 |    50/  196 batches | accuracy    0.501\n",
            "| epoch   9 |   100/  196 batches | accuracy    0.504\n",
            "| epoch   9 |   150/  196 batches | accuracy    0.497\n",
            "| epoch  10 |    50/  196 batches | accuracy    0.500\n",
            "| epoch  10 |   100/  196 batches | accuracy    0.505\n",
            "| epoch  10 |   150/  196 batches | accuracy    0.490\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "EPOCHS = 10 # epoch\n",
        "\n",
        "model = LSTMcustom(word_vec=word_emb, embed_dim=EMBEDDING_DIM).to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "criterion = torch.nn.BCELoss()\n",
        "total_accu = None\n",
        "\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDx2yTsxgpoM",
        "outputId": "7be038ee-a9ff-4c8d-ea57-0c2da93b079e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy    63.92%\n"
          ]
        }
      ],
      "source": [
        "accu_test = evaluate(test_dataloader)\n",
        "print('test accuracy {:8.2f}%'.format(accu_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TDvT909IpV2W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "f98d74dfb5b1c8d8a210bfbef9987017a0fff68a56687bafce354ff2c11e0f6f"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "week12(1).ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}